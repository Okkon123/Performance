面试官您好，我叫左杰，就读于重庆大学计算机科学与技术专业，有一段在哈啰的实习经历，之前还学习过一些国外的公开课，主要的技术栈是Java、Spring、MySQL、Redis、JUC、JVM。性格真诚友善，能与他人建立积极的工作关系，能够在高压环境下保持冷静并有效地处理工作压力，以乐观态度解决问题。平时的爱好是打羽毛球。

在哈啰我们组的主要业务是对全国单车/电动车进行调度，核心目标是把单车从没人骑的地方移到有人骑的地方。
我主要参与的工作一是稳定性的建设，这个又可以分为可观测性和可用性两部分，可观测性就是通过埋点的形式记录核心链路的业务结果，并且通过大盘展示，再通过监控告警及时发现异常。可用性就是对服务内部应用的稳定性进行建设，解决线上问题，产出故障应急方案。二是竞对嗅探的业务需求，这个做的事情就是对车辆下发指令，通过蓝牙模块获取竞争对手车辆数量，为调度提供依据，三是一些小的业务技术需求。
主要的重点和难点是解决线上线程池阻塞队列积压问题，MySQL深分页问题，优化Es接口性能，解决竞对嗅探的时延和有效性。
防止了一次P0事故，几次小事故，让大家定位问题的速度更快了，同时降低了竞对嗅探的链路的时延。

1. 稳定性建设，从可观测性和可用性两个方向出发，预防一次P0事故，几次小事故，解决了线上线上线程池阻塞队列积压问题，MySQL深分页问题，优化Es接口性能等问题。
2. 竞对嗅探需求，对车辆下发指令，通过蓝牙模块获取竞争对手车辆数量，为调度提供依据。
3. 内部业务技术需求，解决高QPS接口治理、Kafka发送超时、依赖升级等问题。

在哈啰我们组的主要业务是对全国单车/助力车进行调度、还有调度助力车的换电，在实习期间我主要有三大块工作
- 第一块是对稳定性的建设，前期的工作主要是建立核心指标的可观测性，对核心链路的结果进行监控告警。防止了几次事故。
- 第二块是一个比较大的需求，这个需求还挺有意思的，是一个和竞争对手的博弈，我们组对是对全国车辆进行调度的，而调度的一个重要根据就是竞对的车辆数。然后我做的事就是定期的对城市的的部分站点的部分车辆下发嗅探指令，车辆上的蓝牙就会嗅探竞对的车辆数然后返回，我拿到这个车辆数以后再交给算法。给他们提供调度的依据。这个博弈还蛮有意思的，我最开始以为各个公司的运维只调度自己的车辆，但我后面发现，美团的运维不仅仅调度美团的车辆，还会把哈啰的车移到偏僻一点的地方。
	- 我解决了几个问题
	- 一是嗅探的时效性和可持续性，之前的方案是对城市所有站点的所有车辆下发嗅探指令，导致返回时间很长，并且车辆的电池一天最多也就只支持16次嗅探，所以嗅探的频率和次数有限制，之前的方案是每天小时下发嗅探指令，而且返回的时间很长，所以数据实效性很差，我改用了每15分钟对城市下部分站点进行嗅探，频率更高，每次的数据量变少了，时效性也更好了，而且以为是对部分车辆下发指令，就解决了多次下发指令后站点车辆都没电的情况，可以一直获取站点下的数据。
	- 从细节上来说，使用了线程池并行消费对城市的嗅探任务，让任务的消费速度更快，同时使用RocketMQ进行削峰填谷，减少瞬时的压力，使用MQ带来的问题一个直接问题就是消息重复投递，所以使用Redis来防止消息被重复消费。
	- 选用MQ的原因
	- 选择RocketMQ的原因
- 第三块就是一些组内的一些需求，比如说高qps治理、es接口优化、依赖升级。

- 一方面是业务结果侧的，建设了核心链路的监控告警，这个主要的原理是应用向Kafka发送埋点信息，flink消费Kafka数据并进行监控指标清洗，生成Metric，Prometheus采集Metric，每个Metric相当于Prometheus的一条数据，每个Metric有一个时间戳，以时序存储，然后再通过PromQL语句查询Prometheus的数据，Grafana再以图表的形式展示数据，最后Prometheus Alters再根据配置的阈值进行告警，假设发送异常，就调用后端的接口通知对应的负责人。在这期间遇到的主要的问题是有很多误告的情况，第一个是因为有些指标的白天和夜间的波动范围相差比较大，这种就需要分两个指标配置，另外一种情况是因为我们只能监控增量的数据，没法监控全量的数据，导致有时候没有请求的时候，就没有埋点的数据，导致误告，最开始只是简单的增加监控的时间，但是越来越长，导致告警没有意义，不能快速的发现问题，后续修改了告警的模式，只有在有请求同时没结果的情况下才进行告警，这个策略大大改善了误告的情况，并且能在五分钟内发现问题，告警上线以后预防了一起事故。
- 另一个方面是核心链路的可用性，这个是我目前在做的一个工作，主要的任务是梳理核心链路对外暴露的接口、下游的接口、还有使用到的一些中间件，比如说redis、db、mq、es、线程池，对这些中间件进行监控告警，并且产出对应的应急方案。
- 再实习期间最有技术含量的工作是对Es的优化，我们有一个接口是召回助力车画像，当时的背景是这个接口的性能不断劣化，风险巡检是的时候发现了这个问题，通知我们进行优化，我首先通过profile分析了一下他线上语句的耗时情况，发现主要耗时集中在一个Integer字段上，我们es的版本用的是5.6.3，在es5系列版本里，对Integer字段进行了优化，不在使用倒排索引的结构构，而是采用Block k-d Tree来存储(类似于B+树的结构，但是多维)，对Integer字段进行termquery时会走范围查询，将符合条件的的数据过滤出来，构建成一个bitset，再与其他条件进行比较，而构建bitset这个过程是非常耗时的，因为数据量比较大，并且两端的数据可能再磁盘块的中间，还需要便利寻找边界。最开始是想讲Integer字段改为keyword字段，这样可以借助倒排索引跳表的特性快速过滤，但是数据迁移的方案比较复杂，需要双写灰度切换，并且涉及到的业务方比较多，所以只在我们这一侧改动，将原先的termQuery改为rangeQuery，在使用rangeQuery时，es做了一层优化当涉及的数据量很多时，他会走indexOrDocValuesQuery，利用es的列存储，不构建庞大的bitset，直接使用之前过滤出来的id在docvalue中查找是否符合要求的值，比如说term1对应结果集5条，term2对应结果集50000条，我不需要把term2对应的文档id转化成bitset再去跟term1的合并，直接遍历term1的文档id，从term2的docvalues中查询文档id对应的term2是多少，满足条件就返回这个文档id，不满足的话这个id就过滤掉。但是在我测试的过程中的效果一直不太明显，优化效果大概只有10%左右，但是我直接使用dsl语句查询Es的时候，优化效果又比较明显，当时我排查了很久，都没发现有什么问题，最后发现是pre环境的机器资源不足的原因，在pre环境压测的时候，单台机器的CPU水位到了70～80%，最后上线以后接口耗时从原来的100ms降低到了60ms，降低了40%，这次的感受就是代码不是悬浮的，他是运行在机器上的，写代码的时候也要关注机器的能力。


- 实习中做的最难的事情
	- 稳定性的建设
- 项目背景
	- 我刚来不久的时候因为算法的原因出现了事故，虽然和我们组无关，但定责波及到了我们组，之前内部只有一些非常粗粒度的告警，所以就让我建立核心链路的监控告警，也就是先建立结果的可观测性，在可观测性建设好以后，再进一步对链路内部的可用性建设，产出了接口和MQ的故障应急方案。
- 技术难点
	- 告警的调整
		- 调整告警为有请求时异常才进行告警，大大减少误告情况
	- 告警响应定位，排查问题
		- 梳理核心链路涉及到的接口和中间件，对核心链路的全貌更加理解
	- 线程池阻塞队列堆积问题
		- 当时出现了线程池队列积压告警，然后去看代码，发现线程池理论上使用的应该是无元素的阻塞队列，不应该出现堆积的情况，但这个线程池之前是使用的无界的阻塞队列，是用开关进行切换的。当时我们应用有七台实例，发现有五台是有堆积的情况的，有两台没有堆积的情况，最开始怀疑可能是开关有什么问题，导致一部分开关生效，一部分开关失效，但是查了很久也没找到问题。这个线程池是消费Kafka数据的，怀疑可能和Kafka有关，再看对应的topic只有5个partition，正好和五个线程池堆积对上了，应该就是五台实例抢占了5个partition，另外两台实际上没有消费消息，所以没有堆积，所以应该是开关根本没有生效，不是一部分生效了，一部分失效了。最后发现是@Value注解的坑，我们线程池的切换是在构造函数里切换的，但是@Value注解在构造函数之后生效，所有开关走的默认值，线程池使用的默认的线程池。
- 项目结果用什么指标评估
	- 问题定位时间
	- 预防事故次数
	- 过程观测能力